{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "denclue.py\n",
    "@author: mgarrett\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "import networkx as nx\n",
    "\n",
    "def _hill_climb(x_t, X, W=None, h=0.1, eps=1e-7):\n",
    "    \"\"\"\n",
    "    This function climbs the 'hill' of the kernel density function\n",
    "    and finds the 'peak', which represents the density attractor\n",
    "    \"\"\"\n",
    "    error = 99.\n",
    "    prob = 0.\n",
    "    x_l1 = np.copy(x_t)\n",
    "    \n",
    "    #Sum of the last three steps is used to establish radius\n",
    "    #of neighborhood around attractor. Authors suggested two\n",
    "    #steps works well, but I found three is more robust to\n",
    "    #noisey datasets.\n",
    "    radius_new = 0.\n",
    "    radius_old = 0.\n",
    "    radius_twiceold = 0.\n",
    "    iters = 0.\n",
    "    while True:\n",
    "        radius_thriceold = radius_twiceold\n",
    "        radius_twiceold = radius_old\n",
    "        radius_old = radius_new\n",
    "        x_l0 = np.copy(x_l1)\n",
    "        x_l1, density = _step(x_l0, X, W=W, h=h)\n",
    "        error = density - prob\n",
    "        prob = density\n",
    "        radius_new = np.linalg.norm(x_l1-x_l0)\n",
    "        radius = radius_thriceold + radius_twiceold + radius_old + radius_new\n",
    "        iters += 1\n",
    "        if iters>3 and error < eps:\n",
    "            break\n",
    "    return [x_l1, prob, radius]\n",
    "\n",
    "def _step(x_l0, X, W=None, h=0.1):\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    superweight = 0. #superweight is the kernel X weight for each item\n",
    "    x_l1 = np.zeros((1,d))\n",
    "    if W is None:\n",
    "        W = np.ones((n,1))\n",
    "    else:\n",
    "        W = W\n",
    "    for j in range(n):\n",
    "        kernel = kernelize(x_l0, X[j], h, d)\n",
    "        kernel = kernel * W[j]/(h**d)\n",
    "        superweight = superweight + kernel\n",
    "        x_l1 = x_l1 + (kernel * X[j])\n",
    "    x_l1 = x_l1/superweight\n",
    "    density = superweight/np.sum(W)\n",
    "    return [x_l1, density]\n",
    "    \n",
    "def kernelize(x, y, h, degree):\n",
    "    kernel = np.exp(-(np.linalg.norm(x-y)/h)**2./2.)/((2.*np.pi)**(degree/2))\n",
    "    return kernel\n",
    "\n",
    "class DENCLUE(BaseEstimator, ClusterMixin):\n",
    "    \"\"\"Perform DENCLUE clustering from vector array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    h : float, optional\n",
    "        The smoothing parameter for the gaussian kernel. This is a hyper-\n",
    "        parameter, and the optimal value depends on data. Default is the\n",
    "        np.std(X)/5.\n",
    "    eps : float, optional\n",
    "        Convergence threshold parameter for density attractors\n",
    "    min_density : float, optional\n",
    "        The minimum kernel density required for a cluster attractor to be\n",
    "        considered a cluster and not noise.  Cluster info will stil be kept\n",
    "        but the label for the corresponding instances will be -1 for noise.\n",
    "        Since what consitutes a high enough kernel density depends on the\n",
    "        nature of the data, it's often best to fit the model first and \n",
    "        explore the results before deciding on the min_density, which can be\n",
    "        set later with the 'set_minimum_density' method.\n",
    "        Default is 0.\n",
    "    metric : string, or callable\n",
    "        The metric to use when calculating distance between instances in a\n",
    "        feature array. In this version, I've only tested 'euclidean' at this\n",
    "        moment.\n",
    "    Attributes\n",
    "    -------\n",
    "    cluster_info_ : dictionary [n_clusters]\n",
    "        Contains relevant information of all clusters (i.e. density attractors)\n",
    "        Information is retained even if the attractor is lower than the\n",
    "        minimum density required to be labelled a cluster.\n",
    "    labels_ : array [n_samples]\n",
    "        Cluster labels for each point.  Noisy samples are given the label -1.\n",
    "    Notes\n",
    "    -----\n",
    "    References\n",
    "    ----------\n",
    "    Hinneburg A., Gabriel HH. \"DENCLUE 2.0: Fast Clustering Based on Kernel \n",
    "    Density Estimation\". In: R. Berthold M., Shawe-Taylor J., LavraÄ N. (eds)\n",
    "    Advances in Intelligent Data Analysis VII. IDA 2007\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h=None, eps=1e-8, min_density=0., metric='euclidean'):    \n",
    "        print('h', h)\n",
    "        self.h = h        \n",
    "        self.eps = eps\n",
    "        self.min_density = min_density\n",
    "        self.metric = metric\n",
    "        \n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        if not self.eps > 0.0:\n",
    "            raise ValueError(\"eps must be positive.\")\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_features = X.shape[1]\n",
    "        density_attractors = np.zeros((self.n_samples,self.n_features))\n",
    "        radii = np.zeros((self.n_samples,1))\n",
    "        density = np.zeros((self.n_samples,1))\n",
    "        \n",
    "        #create default values\n",
    "        if self.h is None:\n",
    "            self.h = np.std(X)/5\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones((self.n_samples,1))\n",
    "        else:\n",
    "            sample_weight = sample_weight\n",
    "        \n",
    "        #initialize all labels to noise\n",
    "        labels = -np.ones(X.shape[0])\n",
    "        \n",
    "        #climb each hill\n",
    "        for i in range(self.n_samples):\n",
    "            density_attractors[i], density[i], radii[i] = _hill_climb(X[i], X, W=sample_weight,\n",
    "                                                     h=self.h, eps=self.eps)\n",
    "        \n",
    "        #initialize cluster graph to finalize clusters. Networkx graph is\n",
    "        #used to verify clusters, which are connected components of the\n",
    "        #graph. Edges are defined as density attractors being in the same\n",
    "        #neighborhood as defined by our radii for each attractor.\n",
    "        cluster_info = {}\n",
    "        num_clusters = 0\n",
    "        cluster_info[num_clusters]={'instances': [0],\n",
    "                                    'centroid': np.atleast_2d(density_attractors[0])}\n",
    "        \n",
    "        g_clusters = nx.Graph()\n",
    "        for j1 in range(self.n_samples):\n",
    "            g_clusters.add_node(j1)\n",
    "            g_clusters.node[j1]['attractor'] = density_attractors[j1]\n",
    "            g_clusters.node[j1]['radius'] = radii[j1]\n",
    "            g_clusters.node[j1]['density'] = density[j1]\n",
    "#=========================================================================================================        \n",
    "        #populate cluster graph\n",
    "        for j1 in range(self.n_samples):\n",
    "            for j2 in (x for x in range(self.n_samples) if x != j1):\n",
    "                if g_clusters.has_edge(j1,j2):\n",
    "                    continue\n",
    "                diff = np.linalg.norm(g_clusters.node[j1]['attractor']-\n",
    "                                      g_clusters.node[j2]['attractor'])\n",
    "                if diff <= (g_clusters.node[j1]['radius']+g_clusters.node[j1]['radius']):\n",
    "                    g_clusters.add_edge(j1, j2)\n",
    "                    \n",
    "        #connected components represent a cluster\n",
    "        clusters = list(nx.connected_component_subgraphs(g_clusters))\n",
    "        num_clusters = 0\n",
    "        \n",
    "        #loop through all connected components\n",
    "        for clust in clusters:\n",
    "            \n",
    "            #get maximum density of attractors and location\n",
    "            max_instance = max(clust, key=lambda x: clust.node[x]['density'])\n",
    "            max_density = clust.node[max_instance]['density']\n",
    "            max_centroid = clust.node[max_instance]['attractor']\n",
    "            \n",
    "            #populate cluster_info dict\n",
    "            cluster_info[num_clusters] = {'instances': clust.nodes(),\n",
    "                                        'size': len(clust.nodes()),\n",
    "                                        'centroid': max_centroid,\n",
    "                                        'density': max_density}\n",
    "            \n",
    "            #if the cluster density is not higher than the minimum,\n",
    "            #instances are kept classified as noise\n",
    "            if max_density >= self.min_density:\n",
    "                labels[clust.nodes()]=num_clusters            \n",
    "            num_clusters += 1\n",
    "            \n",
    "        self.clust_info_ = cluster_info\n",
    "        self.labels_ = labels\n",
    "        return self.clust_info_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3                4\n",
      "0  5.9  3.0  4.2  1.5  Iris-versicolor\n",
      "1  6.9  3.1  4.9  1.5  Iris-versicolor\n",
      "2  6.6  2.9  4.6  1.3  Iris-versicolor\n",
      "3  4.6  3.2  1.4  0.2      Iris-setosa\n",
      "4  6.0  2.2  4.0  1.0  Iris-versicolor\n",
      "5  4.7  3.2  1.3  0.2      Iris-setosa\n",
      "6  6.5  3.0  5.8  2.2   Iris-virginica\n",
      "7  5.8  2.7  5.1  1.9   Iris-virginica\n",
      "8  6.7  3.1  5.6  2.4   Iris-virginica\n",
      "9  6.7  2.5  5.8  1.8   Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('iris.txt', header=None)\n",
    "print(data.head(10))\n",
    "data1 = np.array(data)\n",
    "iris = np.mat(data1[:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h 0.37\n",
      "{0: {'instances': NodeView((0, 4, 139, 12, 17, 146, 23, 24, 26, 33, 34, 35, 36, 44, 47, 50, 59, 64, 65, 72, 81, 85, 96, 98, 110, 114, 126)), 'size': 27, 'centroid': array([5.91118336, 2.84759438, 4.37154782, 1.37436614]), 'density': array([0.13773153])}, 1: {'instances': NodeView((128, 1, 129, 2, 131, 132, 6, 7, 8, 9, 137, 138, 140, 13, 141, 142, 143, 144, 19, 20, 21, 147, 148, 28, 29, 32, 38, 39, 40, 41, 42, 43, 46, 51, 53, 55, 56, 57, 58, 60, 61, 66, 70, 71, 77, 79, 80, 82, 83, 84, 86, 87, 89, 92, 93, 94, 95, 97, 99, 100, 101, 102, 103, 104, 107, 112, 116, 117, 118, 119, 120, 122, 123)), 'size': 73, 'centroid': array([6.18638554, 2.89567907, 4.72334636, 1.56653329]), 'density': array([0.14536436])}, 2: {'instances': NodeView((130, 3, 5, 134, 133, 135, 136, 10, 11, 14, 15, 16, 145, 18, 149, 22, 25, 27, 30, 31, 37, 45, 48, 49, 52, 54, 62, 63, 67, 68, 69, 73, 74, 75, 76, 78, 88, 90, 91, 105, 106, 108, 109, 111, 113, 115, 121, 124, 125, 127)), 'size': 50, 'centroid': array([4.97574274, 3.35673513, 1.47789821, 0.23719418]), 'density': array([0.22121843])}}\n",
      "                                           instances  size  \\\n",
      "0  (0, 4, 139, 12, 17, 146, 23, 24, 26, 33, 34, 3...    27   \n",
      "1  (128, 1, 129, 2, 131, 132, 6, 7, 8, 9, 137, 13...    73   \n",
      "2  (130, 3, 5, 134, 133, 135, 136, 10, 11, 14, 15...    50   \n",
      "\n",
      "                                            centroid                density  \n",
      "0  [5.911183360364738, 2.8475943828248926, 4.3715...  [0.13773153156532322]  \n",
      "1  [6.186385542088264, 2.8956790693440864, 4.7233...  [0.14536435952332374]  \n",
      "2  [4.975742735239105, 3.3567351285343463, 1.4778...  [0.22121842695653518]  \n"
     ]
    }
   ],
   "source": [
    "den = DENCLUE(h=0.37, eps=0.0001, min_density=0.13, metric='euclidean')\n",
    "clusters = den.fit(iris, y=None, sample_weight=None)\n",
    "print(clusters)\n",
    "\n",
    "clu = pd.DataFrame.from_dict(clusters, orient='index')\n",
    "print(clu)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity =  0.83\n"
     ]
    }
   ],
   "source": [
    "major_class = 0\n",
    "t_size = 0\n",
    "for clust in clusters:\n",
    "    n_ver = 0\n",
    "    n_set = 0\n",
    "    n_vir = 0\n",
    "    for index, row in data.iterrows():\n",
    "        if index in clusters[clust]['instances']:\n",
    "            if row[4] == 'Iris-versicolor':\n",
    "                n_ver += 1\n",
    "            elif row[4] == 'Iris-setosa':\n",
    "                n_set += 1\n",
    "            elif row[4] == 'Iris-virginica':\n",
    "                n_vir += 1\n",
    "\n",
    "    major_class += max(n_ver, n_set, n_vir)\n",
    "    t_size += clusters[clust]['size']\n",
    "\n",
    "print('Purity = ', round(major_class/t_size, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
